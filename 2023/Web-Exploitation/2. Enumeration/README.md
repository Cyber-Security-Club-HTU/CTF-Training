
# Enumeration

## Fuzzing

Fuzzing is the process of injecting various invalid or unexpected input into a software in order to expose vulnerabilities. *Directory Fuzzing* is a process we start in order to expose paths and files that we never knew existed, and it involves the use of automated tools such as `dirb`, `gobuster`, and more importantly: `ffuf`.

`dirb` is a tool that comes pre-installed in Kali Linux, and it works in the Command-Line Interface. In case you are using a different distro, run this command `sudo apt install dirb`.

To tell `dirb` to fuzz a certain URL, we issue this command:

```bash
dirb http://example.com </path/to/words.txt>
```

As you can see, `dirb` requires a wordlist to work. There are many well-curated and large wordlists that can be used to crack many passwords and guess various URLs. We have 2 options for choosing the wordlist:

1. Using very good preinstalled wordlist, in Kali Linux, you fill find here: `/usr/share/wordlists/dirb/common.txt`
2. Installing "SecLists", which is an amazing collection of wordlists that cover many scenarios.

Lets demonstrate how to use "SecLists". To install it, issue this command:

```bash
apt -y install seclists
```

>If you prefer not to install the whole list (since it is very big), install the desired wordlists manually from [here](https://github.com/danielmiessler/SecLists).

When the command is done installing, you will find a directory with all the wordlists that *SecLists* provides to us, which is by default `usr/share/seclists`. Within this directory you will find more folders that separate wordlists by type. We will use the ones in *Web-Content*. For our purposes, we will use `common.txt` in our fuzzing process.

Now try `dirb` on any site you want.

```bash
dirb https://github.com/ /usr/share/seclists/Web-Content/common.txt
```

It will take some time, but soon it will list all the different directories and files it can find, whether we can access them or not.

> An important file to keep an eye out for is `robots.txt`. This file is used to tell search engine web crawlers what directories to ignore, but it can also contain some hints that are useful to us or even flags.

---

## Requested Files

We can use the Network tab in the browser's developer tools to see what files or content does the site page request. It can contain valuable information for us. It could be JavaScript, data requests and even GET and POST requests the page makes.

> Hit refresh when you open the tab to see these files.

---

## Files Content

It is vital that we always check the source code available to us through the developer tools. `.html`, `.css`, and `.js` files can contain comments, which are plain text that can contain valuable information. Or even hidden functionality that can lead to a vulnerability that we can exploit.

**Comments syntax in each language:**

- HTML: `<!-- content -->`
- CSS & JS: `// content`

**Such information we can expect are:**

- **PHP requests:** such as `?example` or `?id=1` or `?admin=true`. We can append these to URLs and we might get access to new content.

- **Web framework information:** Sites can use frameworks and external services to build their frontend, and knowing these frameworks can help us exploit existing vulnerabilities that are related to said frameworks.

- **Links to undiscovered pages:** Links and references to other files can be found, such as CSS and JavaScript files. And lastly, HTML tags have classes that describe what elements do, and we can tamper with them and possibly reveal hidden content or force the page to act abnormally.

---

## Cookies

*Cookies* are key-value pairs that a website publicly stores and uses.

We can change these cookies and even create our own cookies. **To do this:**

1. go to the developer tools, then to storage, then cookies.
2. We might find cookies and try to alter them by double-clicking the *key* or the *value*.

This can grant us extra privileges and access to previously hidden content.

---

## Useful Extensions

- **Wappalyzer:** It will gives you all the web framework information the website has. This will save us time and give us insights on what are we dealing with.
- **Cookie-Editor:** Another way to edit cookies can be using this browser extension on Firefox.
